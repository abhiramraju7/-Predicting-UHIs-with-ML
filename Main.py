# -*- coding: utf-8 -*-
"""Working Group 4- Building the models for predicting UHIs.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1w9oAACcw0AQfIZu5oEo1rkzRHXjcorHy

##Data Loading:
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn.preprocessing import StandardScaler
import seaborn as sns

from google.colab import drive
drive.mount('/content/drive')

file_path= "/content/drive/MyDrive/FINALUHIDATA.csv"
data=pd.read_csv(file_path)

"""##Data Preprocessing:"""

data.head()

# Selecting required columns
columns = ['longitude', 'latitude', 'grass', 'Impervious Surface (no building)', 'Trees_CanopyCover', 'bush/scrub', 'Median Income_LST', 'Average Income_LST']#,'ECO_L2T_LSTE_002_LST']
data_selected = data.loc[:, columns]

# Combine grass, Trees_CanopyCover, and bush/scrub into a single variable
data_selected.loc[:, 'Vegetation'] = data_selected[['grass', 'Trees_CanopyCover', 'bush/scrub']].sum(axis=1)

# Remove the individual columns
data_selected = data_selected.drop(columns=['grass', 'Trees_CanopyCover', 'bush/scrub'])

# Display the first few rows of the modified dataset
data_selected.head()

# Remove rows with missing values
data_cleaned = data_selected.dropna()

# Display the first few rows of the cleaned dataset
data_cleaned.head()

data_cleaned.info()

data_cleaned.rename(columns={'Impervious Surface (no building)': 'Impervious'}, inplace=True)
data_cleaned.rename(columns={'Median Income_LST': 'Median_Income'}, inplace=True)
data_cleaned.rename(columns={'Average Income_LST': 'Avg_Income'}, inplace=True)
data_cleaned.rename(columns={'ECO_L2T_LSTE_002_LST': 'LST'}, inplace=True)

from sklearn.preprocessing import StandardScaler

# Normalize the data
scaler = StandardScaler()
data_normalized = scaler.fit_transform(data_cleaned)

# Convert normalized data back to DataFrame
data_normalized = pd.DataFrame(data_normalized, columns=data_cleaned.columns)

# Display the first few rows of the normalized dataset
data_normalized.head()

data_normalized.head()

data_normalized.info()

"""#Income"""

#Distribution of median incomes

plt.figure(figsize=(10, 6))
data_cleaned['Median_Income'].dropna().astype(float).hist(bins=50, edgecolor='black')

plt.title('Distribution of Median Incomes')
plt.xlabel('Median Income')
plt.ylabel('Frequency')
plt.grid(False)
plt.show()

#Distribution of minimum distances

plt.figure(figsize=(10, 6))

data['Min Distance'].dropna().hist(bins=50, edgecolor='black')

plt.title('Frequency Distribution of Minimum Distances')
plt.xlabel('Minimum Distance (meters)')
plt.ylabel('Frequency')
plt.grid(False)
plt.show()

"""### Vegetation and Impervious"""

# Boxplot for Vegetation and Impervious
plt.figure(figsize=(12, 6))
sns.boxplot(data=data_cleaned[['Vegetation', 'Impervious']])
plt.title('Boxplot for Vegetation and Impervious Surface')
plt.ylabel('Value')
plt.show()

# Histogram for Vegetation
plt.figure(figsize=(12, 6))
sns.histplot(data=data_cleaned, x='Vegetation', kde=True, color='green')
plt.title('Histogram for Vegetation')
plt.xlabel('Vegetation')
plt.ylabel('Frequency')
plt.show()

# Histogram for Impervious Surface
plt.figure(figsize=(12, 6))
sns.histplot(data=data_cleaned, x='Impervious', kde=True, color='red')
plt.title('Histogram for Impervious Surface')
plt.xlabel('Impervious Surface')
plt.ylabel('Frequency')
plt.show()

"""#LSTS"""

lsts= data['ECO_L2T_LSTE_002_LST']
print(lsts.mean())
aoi_avg_lst = data.groupby(['latitude', 'longitude'])['ECO_L2T_LSTE_002_LST'].mean().reset_index()
print(aoi_avg_lst)

plt.figure(figsize=(10, 6))

plt.scatter(data['Median Income_LST'], lsts, alpha=0.5, c='blue')
plt.xlabel('Median Income')
plt.ylabel('Average LST')
plt.title('Median Income vs. Average LST')
plt.show()

plt.figure(figsize=(12, 6))
sns.boxplot(data=data_cleaned['LST'])
plt.title('Boxplot for Vegetation and Impervious Surface')
plt.ylabel('Value')
plt.show()

"""#Model Training: K-Means

Clustering:

K-means:
"""

from sklearn.cluster import KMeans
from sklearn.metrics import silhouette_score, davies_bouldin_score, calinski_harabasz_score
# Define the range of cluster numbers to evaluate
cluster_numbers = [2, 3, 4, 5]

# Initialize lists to store the scores
silhouette_scores = []
davies_bouldin_scores = []
calinski_harabasz_scores = []

# Evaluate the metrics for each number of clusters
for n_clusters in cluster_numbers:
    kmeans = KMeans(n_clusters=n_clusters, n_init=10)
    cluster_labels = kmeans.fit_predict(data_normalized)

    silhouette_avg = silhouette_score(data_normalized, cluster_labels)
    davies_bouldin_avg = davies_bouldin_score(data_normalized, cluster_labels)
    calinski_harabasz_avg = calinski_harabasz_score(data_normalized, cluster_labels)

    silhouette_scores.append(silhouette_avg)
    davies_bouldin_scores.append(davies_bouldin_avg)
    calinski_harabasz_scores.append(calinski_harabasz_avg)

# Plot the evaluation metrics
plt.figure(figsize=(14, 8))

plt.subplot(3, 1, 1)
plt.plot(cluster_numbers, silhouette_scores, marker='o')
plt.title('Silhouette Score for Different Numbers of Clusters')
plt.xlabel('Number of Clusters')
plt.ylabel('Silhouette Score')

plt.subplot(3, 1, 2)
plt.plot(cluster_numbers, davies_bouldin_scores, marker='o', color='orange')
plt.title('Davies-Bouldin Index for Different Numbers of Clusters')
plt.xlabel('Number of Clusters')
plt.ylabel('Davies-Bouldin Index')

plt.subplot(3, 1, 3)
plt.plot(cluster_numbers, calinski_harabasz_scores, marker='o', color='green')
plt.title('Calinski-Harabasz Index for Different Numbers of Clusters')
plt.xlabel('Number of Clusters')
plt.ylabel('Calinski-Harabasz Index')

plt.tight_layout()
plt.show()

from sklearn.cluster import KMeans

# Define the model with n_init parameter for 3 clusters
kmeans = KMeans(n_clusters=2, n_init=10)

# Fit the model
kmeans.fit(data_normalized)

# Predict the clusters and add to data_cleaned
data_cleaned['Cluster'] = kmeans.predict(data_normalized)

data_cleaned.head()

# Ensure only numeric columns are selected for analysis
numeric_columns = data_cleaned.select_dtypes(include=[float, int]).columns

# Analyze the clusters
cluster_analysis = data_cleaned.groupby('Cluster')[numeric_columns].mean()
print(cluster_analysis)

# Define labels for clusters (assuming cluster 0 is low risk, 1 is medium risk, and 2 is high risk based on analysis)
labels = {1: 'Low Risk', 0: 'High Risk'}

# Map the labels
data_cleaned['Risk'] = data_cleaned['Cluster'].map(labels)

# Visualize the clusters with labels
plt.figure(figsize=(10, 6))
colors = {'Low Risk': 'purple', 'Medium Risk': 'blue', 'High Risk': 'yellow'}
plt.scatter(data_cleaned['longitude'], data_cleaned['latitude'], c=data_cleaned['Risk'].apply(lambda x: colors[x]), marker='o')
plt.title('K-means Clustering of Urban Heat Islands')
plt.xlabel('Longitude')
plt.ylabel('Latitude')
plt.legend(handles=[plt.Line2D([0], [0], marker='o', color='w', markerfacecolor=colors[label], markersize=10, label=label) for label in labels.values()],
           title='Risk Level')
plt.show()

# Bar plot for Median Income and Average Income
income_features = ['Median_Income', 'Avg_Income']
income_means = data_cleaned.groupby('Risk')[income_features].mean()

income_means.plot(kind='bar', figsize=(12, 8))
plt.title('Mean Income Values by Risk Level')
plt.ylabel('Mean Value')
plt.xlabel('Risk Level')
plt.xticks(rotation=0)
plt.show()

# Bar plot for Impervious Surface and Vegetation
environmental_features = ['Impervious', 'Vegetation']
environmental_means = data_cleaned.groupby('Risk')[environmental_features].mean()

environmental_means.plot(kind='bar', figsize=(12, 8))
plt.title('Mean Environmental Values by Risk Level')
plt.ylabel('Mean Value')
plt.xlabel('Risk Level')
plt.xticks(rotation=0)
plt.show()

# Quality of k-means model
# Calculate the silhouette score
sil_score = silhouette_score(data_normalized, kmeans.labels_)
print(f'Silhouette Score: {sil_score}')



"""#Model Training: DBSCAN

Clustering: DBSCAN
"""

# Remove rows with missing values
data_cleaned = data_selected.dropna()

# Display the first few rows of the cleaned dataset
data_cleaned.head()

data_cleaned.info()

from sklearn.preprocessing import StandardScaler
from sklearn.neighbors import NearestNeighbors

# Normalize the data
scaler = StandardScaler()
data_normalized = scaler.fit_transform(data_cleaned)

# Convert normalized data back to DataFrame
data_normalized = pd.DataFrame(data_normalized, columns=data_cleaned.columns)

# Display the first few rows of the normalized dataset
data_normalized.head()

# Fit the model
nearest_neighbors = NearestNeighbors(n_neighbors=5)
neighbors = nearest_neighbors.fit(data_normalized)
distances, indices = neighbors.kneighbors(data_normalized)

# Sort the distances to the 5th nearest neighbor - helpful for better clustering results
distances = np.sort(distances, axis=0)
distances = distances[:, 4]

# Plot the distances
plt.figure(figsize=(10, 6))
plt.plot(distances)
plt.title('K-distance Graph')
plt.xlabel('Points sorted by distance to 5th nearest neighbor')
plt.ylabel('5th nearest neighbor distance')
plt.show()

from sklearn.cluster import DBSCAN

# apply DBSCAN
dbscan = DBSCAN(eps=1.467, min_samples=4)
# 1.02 determined by iterating by 0.01 from 0.7 to 1.5 by iteratively calling
# function for highest silhouette score
# eps value is found from graph above - can adjust to tune for results
data_cleaned['Cluster'] = dbscan.fit_predict(data_normalized)

cluster_analysis = data_cleaned.groupby('Cluster').mean()

# adjusting the label mapping to include all detected clusters
unique_clusters = data_cleaned['Cluster'].unique()
labels = {cluster: f'Cluster {cluster}' for cluster in unique_clusters}
labels[-1] = 'Noise'  # assigning 'Noise' label to -1 cluster

# map the labels
data_cleaned['Risk'] = data_cleaned['Cluster'].map(labels)

# visualize the clusters with labels
plt.figure(figsize=(10, 6))
colors = {f'Cluster {cluster}': plt.cm.tab20(i) for i, cluster in enumerate(unique_clusters)}
colors['Noise'] = 'red'
plt.scatter(data_cleaned['longitude'], data_cleaned['latitude'], c=data_cleaned['Risk'].apply(lambda x: colors[x]), marker='o')
plt.title('DBSCAN Clustering of Urban Heat Islands')
plt.xlabel('Longitude')
plt.ylabel('Latitude')
plt.legend(handles=[plt.Line2D([0], [0], marker='o', color='w', markerfacecolor=colors[label], markersize=10, label=label) for label in labels.values()],
           title='Cluster')
plt.show()

# bar plot for Median Income and Average Income
income_features = ['Median Income_LST', 'Average Income_LST']
income_means = data_cleaned.groupby('Risk')[income_features].mean()

income_means.plot(kind='bar', figsize=(12, 8))
plt.title('Mean Income Values by Cluster')
plt.ylabel('Mean Value')
plt.xlabel('Cluster')
plt.xticks(rotation=0)
plt.show()

# bar plot for Impervious Surface and Vegetation
environmental_features = ['Impervious Surface (no building)', 'Vegetation']
environmental_means = data_cleaned.groupby('Risk')[environmental_features].mean()

environmental_means.plot(kind='bar', figsize=(12, 8))
plt.title('Mean Environmental Values by Cluster')
plt.ylabel('Mean Value')
plt.xlabel('Cluster')
plt.xticks(rotation=0)
plt.show()

#quality of k-means model
#a score closer to 1 means that the clusters are well separated
#this is how we can compare the models
from sklearn.metrics import silhouette_score

# Calculate the silhouette score
sil_score = silhouette_score(data_normalized, dbscan.labels_)
sil_score

"""K-means clusters are moderately well-separated based on the silhouette score."""



"""#Model Training- Hierarchical

Clustering-- Hierarchical
"""

from sklearn.cluster import AgglomerativeClustering



d=data_cleaned #Data frame

plt.figure(figsize=(12, 6))
sns.scatterplot(x='Impervious', y='Median_Income',  data=d, palette='viridis')
plt.title(f'Impervious Surface vs. Median Income')
plt.xlabel('Impervious Surface')
plt.ylabel('Median Income')
plt.show()

plt.figure(figsize=(12, 6))
sns.scatterplot(x='Impervious', y='Avg_Income', data=d, palette='viridis')
plt.title(f'Impervious Surface vs. Average Income')
plt.xlabel('Impervious Surface')
plt.ylabel('Average Income')
plt.show()

plt.figure(figsize=(12, 6))
sns.scatterplot(x='Vegetation', y='Median_Income', data=d, palette='viridis')
plt.title(f'Greenness vs. Median Income')
plt.xlabel('Greenness')
plt.ylabel('Median Income')
plt.show()

plt.figure(figsize=(12, 6))
sns.scatterplot(x='Vegetation', y='Avg_Income', data=d, palette='viridis')
plt.title(f'Greenness vs. Average Income ')
plt.xlabel('Greenness')
plt.ylabel('Average Income')
plt.show()

plt.figure(figsize=(12, 6))
sns.scatterplot(x='Vegetation', y='Impervious', data=d, palette='viridis')
plt.title(f'Greenness vs. Impervious Surface ')
plt.xlabel('Greenness')
plt.ylabel('Impervious Surface')
plt.show()

clustered_dataframes = {}
data_clust_temp= data_cleaned.copy()

for i in range(1, 6):
    clustering = AgglomerativeClustering(n_clusters=i, linkage='ward').fit(data_normalized)
    data_clust_temp[f'Cluster_{i}'] = clustering.labels_

    clustered_dataframes[i] = data_clust_temp[['Median_Income', 'Avg_Income', 'Impervious', 'Vegetation', 'longitude', 'latitude', f'Cluster_{i}']].copy()
    clustered_dataframes[i].rename(columns={f'Cluster_{i}': 'Cluster'}, inplace=True)

    plt.figure(figsize=(10, 6))
    sns.scatterplot(x=data_cleaned['longitude'], y=data_cleaned['latitude'], hue=clustered_dataframes[i][f'Cluster'], palette='viridis')
    plt.title(f'Hierarchical Clustering of AOI Points - {i} clusters')
    plt.xlabel('Longitude')
    plt.ylabel('Latitude')
    plt.legend(title='Cluster')
    plt.show()

"""Agglomterative/Hierarchical clustering doesnt support noise detection and handling like DBSCAN"""

clustered_dataframes[4]

for i in range(1, 6):
    cluster_data = clustered_dataframes[i]

    plt.figure(figsize=(12, 6))

    sns.pairplot(cluster_data, hue='Cluster', vars=['Median_Income', 'Avg_Income', 'Impervious', 'Vegetation'], palette='viridis')
    plt.suptitle(f'Pairplot for Cluster Insights - {i} Clusters', y=1.02)
    plt.show()

"""for i in range(1, 6):
    cluster_data = clustered_dataframes[i]

    plt.figure(figsize=(12, 6))

    cluster_means = cluster_data.groupby('Cluster').mean()

    cluster_means[['LST']].plot(kind='bar', figsize=(14, 7))

    plt.title(f'LST Values by Cluster {i}')

    plt.ylabel('Mean Value')
    plt.xlabel('Cluster')
    plt.xticks(rotation=0)


    plt.show()"""

for i in range(1, 6):
    cluster_data = clustered_dataframes[i]

    plt.figure(figsize=(12, 6))

    cluster_means = cluster_data.groupby('Cluster').mean()

    cluster_means[['Median_Income', 'Avg_Income']].plot(kind='bar', figsize=(14, 7))

    plt.title(f'Mean Income Values by Cluster {i}')

    plt.ylabel('Mean Value')
    plt.xlabel('Cluster')
    plt.xticks(rotation=0)


    plt.show()

for i in range(1, 6):
    cluster_data = clustered_dataframes[i]

    plt.figure(figsize=(12, 6))

    cluster_means = cluster_data.groupby('Cluster').mean()

    cluster_means[['Impervious', 'Vegetation']].plot(kind='bar', figsize=(14, 7))

    plt.title(f'Environmental values by Cluster {i}')
    plt.ylabel('Percentage')
    plt.xlabel('Cluster')
    plt.xticks(rotation=0)

    plt.show()

"""evaluating the cluster"""

from sklearn.metrics import davies_bouldin_score, calinski_harabasz_score, silhouette_score

cluster_numbers = [2, 3, 4, 5]

silhouette_scores = []
davies_bouldin_scores = []
calinski_harabasz_scores = []

for n_clusters in cluster_numbers:
    cluster_data = clustered_dataframes[n_clusters]
    cluster_labels = cluster_data['Cluster']

    silhouette_avg = silhouette_score(data_normalized, cluster_labels)
    davies_bouldin_avg = davies_bouldin_score(data_normalized, cluster_labels)
    calinski_harabasz_avg = calinski_harabasz_score(data_normalized, cluster_labels)

    silhouette_scores.append(silhouette_avg)
    davies_bouldin_scores.append(davies_bouldin_avg)
    calinski_harabasz_scores.append(calinski_harabasz_avg)

plt.figure(figsize=(14, 8))

plt.subplot(3, 1, 1)
plt.plot(cluster_numbers, silhouette_scores, marker='o')
plt.title('Silhouette Score for Different Numbers of Clusters')
plt.xlabel('Number of Clusters')
plt.ylabel('Silhouette Score')

plt.subplot(3, 1, 2)
plt.plot(cluster_numbers, davies_bouldin_scores, marker='o', color='orange')
plt.title('Davies-Bouldin Index for Different Numbers of Clusters')
plt.xlabel('Number of Clusters')
plt.ylabel('Davies-Bouldin Index')

plt.subplot(3, 1, 3)
plt.plot(cluster_numbers, calinski_harabasz_scores, marker='o', color='green')
plt.title('Calinski-Harabasz Index for Different Numbers of Clusters')
plt.xlabel('Number of Clusters')
plt.ylabel('Calinski-Harabasz Index')

plt.tight_layout()

plt.show()

"""Higher the Silhoutte and C-H indexes, the better defined the clusters are and
Lower the D-B index, the better defined the clusters are

#Anomaly Detection
"""

from sklearn.ensemble import IsolationForest

model = IsolationForest(contamination=0.05, random_state=42)
model.fit(data_normalized)

data_normalized['anomaly'] = model.predict(data_normalized)
data_normalized['anomaly'] = data_normalized['anomaly'].map({1: 0, -1: 1})

num_anomalies = data_normalized['anomaly'].sum()
num_anomalies

data_normalized.head()

a=data_normalized[data_normalized['anomaly'] == 1]

plt.figure(figsize=(10, 6))

plt.scatter(data_normalized['longitude'], data_normalized['latitude'], c=data_normalized['anomaly'], cmap='coolwarm', marker='o', alpha=0.6)
plt.xlabel('Longitude')
plt.ylabel('Latitude')
plt.title('Anomalies Detection Scatter Plot')
plt.colorbar(label='Anomaly')
plt.show()

plt.figure(figsize=(10, 6))
sns.scatterplot(x=data_cleaned['longitude'], y=data_cleaned['latitude'], hue=a['anomaly'], palette='pastel')
plt.title(f'Anomaly lats and longs')
plt.xlabel('Longitude')
plt.ylabel('Latitude')
plt.legend(title='Cluster')
plt.show()

"""#Correlation Tests"""

import pandas as pd
from scipy.stats import pearsonr
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.preprocessing import StandardScaler

from google.colab import drive
drive.mount('/content/drive')

file_path = "/content/drive/MyDrive/FINALUHIDATA.csv"
df = pd.read_csv(file_path)

# Sum total greenness
df['Total Greeness'] = df[['grass', 'cultivated vegetation', 'Trees_CanopyCover', 'bush/scrub']].sum(axis=1)

variables = ['ECO_L2T_LSTE_002_LST', 'Median Income_LST', 'Total Greeness', 'Impervious Surface (no building)']

scaler = StandardScaler()

# Normalize the columns
df[variables] = scaler.fit_transform(df[variables])


# Key factors

# Calculate correlations and p-values ignoring NaN values pairwise
# DON'T SWITCH BACK TO DROP ALL NaN ROWS BECAUSE THAT PREVENTS COMPARISON
# OF CERTAIN ROWS THAT STILL HAVE DATA THAT CAN BE COMPARED IN IT
results = {}
for var1 in variables:
    for var2 in variables:
        if var1 != var2:
            clean_data = df[[var1, var2]].dropna()
            corr, p_value = pearsonr(clean_data[var1], clean_data[var2])
            results[f"{var1} with {var2}"] = (corr, p_value)

# Printing all combinations
for key, (corr, p_value) in results.items():
    print(f"{key}: Correlation = {corr}, p-value = {p_value}", end=' ')
    if p_value < 0.05:
        print("Statistically significant", end='')
    print(end='\n\n')

# Heat map rows
heatmap_data = df[variables]

# Correlation matrix
correlation_matrix = heatmap_data.corr()

# Create heatmap
plt.figure(figsize=(10, 8))
sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt='.3f', square=True, vmin=-1, vmax=1, center=0)
plt.title('Correlation Heatmap')
plt.tight_layout()
plt.savefig('/content/drive/MyDrive/correlation_heatmap.png')
plt.show()